"""
BDDãƒ†ã‚¹ãƒˆ: PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆæ©Ÿèƒ½

ãƒ†ã‚¹ãƒˆå¯¾è±¡:
- ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹
- Given-When-Thenã‚·ãƒŠãƒªã‚ª
"""

import pytest
import json
import tempfile
import os
import logging
from pr_comment_generator.models import PRInfo, FileChange
from pr_comment_generator.statistics import PRCommentStatistics
from pr_comment_generator.formatter import CommentFormatter
from pr_comment_generator.token_estimator import TokenEstimator
from pr_comment_generator.prompt_manager import PromptTemplateManager


class TestPRCommentGenerationBDD:
    """PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆæ©Ÿèƒ½ã®BDDãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def logger(self):
        """ãƒ­ã‚¬ãƒ¼ã‚’ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã¨ã—ã¦æä¾›"""
        return logging.getLogger("test_bdd")

    @pytest.fixture
    def temp_template_dir(self):
        """ä¸€æ™‚ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        with tempfile.TemporaryDirectory() as tmpdir:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            base_template = os.path.join(tmpdir, "base_template.md")
            with open(base_template, "w", encoding="utf-8") as f:
                f.write("ä»¥ä¸‹ã®PRã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š\n\nPR #{pr_number}: {title}\nè‘—è€…: {author}\n")

            chunk_template = os.path.join(tmpdir, "chunk_analysis_extension.md")
            with open(chunk_template, "w", encoding="utf-8") as f:
                f.write("ãƒãƒ£ãƒ³ã‚¯ {chunk_index} ã®åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")

            summary_template = os.path.join(tmpdir, "summary_extension.md")
            with open(summary_template, "w", encoding="utf-8") as f:
                f.write("ä»¥ä¸‹ã®åˆ†æã‹ã‚‰ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")

            yield tmpdir

    def test_scenario_å°è¦æ¨¡PRã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ(self, logger, temp_template_dir):
        """
        Scenario: å°è¦æ¨¡PRã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ

        Given: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPRæƒ…å ±JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ã„ã‚‹
          And: PRæƒ…å ±ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã‚‹:
               - number: 123
               - title: "Add new feature"
               - author: "testuser"
          And: å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã¯3å€‹ã§ã€åˆè¨ˆ100è¡Œã®å¤‰æ›´ã§ã‚ã‚‹
        When: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
        Then: æœ€çµ‚çš„ãªã‚³ãƒ¡ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹
          And: ã‚³ãƒ¡ãƒ³ãƒˆã«"# å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼"ãƒ˜ãƒƒãƒ€ãƒ¼ãŒå«ã¾ã‚Œã‚‹
          And: ã‚³ãƒ¡ãƒ³ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹
        """
        # Given: PRæƒ…å ±ã‚’æº–å‚™
        pr_info = PRInfo.from_json({
            "title": "Add new feature",
            "number": 123,
            "body": "This PR adds a new feature to the system",
            "user": {"login": "testuser"},
            "base": {"ref": "main", "sha": "abc123"},
            "head": {"ref": "feature-branch", "sha": "def456"}
        })

        # And: å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™ï¼ˆ3å€‹ã€åˆè¨ˆ100è¡Œï¼‰
        files = [
            FileChange(
                filename="src/main.py",
                status="modified",
                additions=20,
                deletions=10,
                changes=30,
                patch="@@ -1,10 +1,20 @@\n# Changes here"
            ),
            FileChange(
                filename="src/utils.py",
                status="added",
                additions=50,
                deletions=0,
                changes=50,
                patch="@@ -0,0 +1,50 @@\n# New file"
            ),
            FileChange(
                filename="README.md",
                status="modified",
                additions=15,
                deletions=5,
                changes=20,
                patch="@@ -1,5 +1,15 @@\n# Documentation updates"
            )
        ]

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æº–å‚™
        token_estimator = TokenEstimator(logger=logger)
        statistics = PRCommentStatistics(token_estimator=token_estimator, logger=logger)
        formatter = CommentFormatter(logger=logger)
        prompt_manager = PromptTemplateManager(template_dir=temp_template_dir)

        # When: ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ã‚’å®Ÿè¡Œ
        # Step 1: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        chunk_size = statistics.calculate_optimal_chunk_size(files, max_tokens=3000)

        # Step 2: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [files[i:i + chunk_size] for i in range(0, len(files), chunk_size)]

        # Step 3: çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        stats = statistics.calculate_statistics(files)

        # Step 4: ãƒãƒ£ãƒ³ã‚¯åˆ†æï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        chunk_analyses = [
            f"ãƒãƒ£ãƒ³ã‚¯ {i+1}: {len(chunk)} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã—ãŸã€‚"
            for i, chunk in enumerate(chunks)
        ]

        # Step 5: ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        summary = f"ã“ã®PRã¯ {pr_info.title} ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚{stats['file_count']} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã€åˆè¨ˆ {stats['total_changes']} è¡Œã®å¤‰æ›´ãŒã‚ã‚Šã¾ã™ã€‚"

        # Step 6: æœ€çµ‚ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        comment = formatter.format_final_comment(summary, chunk_analyses, files)

        # Then: ã‚³ãƒ¡ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹
        assert comment is not None
        assert len(comment) > 0

        # And: "# å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼"ãƒ˜ãƒƒãƒ€ãƒ¼ãŒå«ã¾ã‚Œã‚‹
        assert "# å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼" in comment

        # And: ã‚µãƒãƒªãƒ¼ãŒå«ã¾ã‚Œã‚‹
        assert summary in comment

        # And: ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹
        assert "## å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«" in comment
        assert "src/main.py" in comment
        assert "src/utils.py" in comment
        assert "README.md" in comment

        # And: å¤‰æ›´è¡Œæ•°ãŒå«ã¾ã‚Œã‚‹
        assert "+20" in comment
        assert "+50" in comment

    def test_scenario_å¤§è¦æ¨¡PRã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ_ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²(self, logger, temp_template_dir):
        """
        Scenario: å¤§è¦æ¨¡PRã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼‰

        Given: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPRæƒ…å ±JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ã„ã‚‹
          And: PRæƒ…å ±ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã‚‹:
               - number: 456
               - title: "Major refactoring"
               - author: "developer"
          And: å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã¯50å€‹ã§ã€åˆè¨ˆ5000è¡Œã®å¤‰æ›´ã§ã‚ã‚‹
          And: ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯1000è¡Œã‚’è¶…ãˆã‚‹å¤‰æ›´ãŒã‚ã‚‹
        When: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
        Then: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã‚‹
          And: å„ãƒãƒ£ãƒ³ã‚¯ãŒå€‹åˆ¥ã«åˆ†æã•ã‚Œã‚‹
          And: ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ãŒã‚³ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹
        """
        # Given: PRæƒ…å ±ã‚’æº–å‚™
        pr_info = PRInfo.from_json({
            "title": "Major refactoring",
            "number": 456,
            "body": "This PR refactors the entire codebase",
            "user": {"login": "developer"},
            "base": {"ref": "develop", "sha": "xyz789"},
            "head": {"ref": "refactor-branch", "sha": "uvw012"}
        })

        # And: å¤§é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’æº–å‚™ï¼ˆ50å€‹ï¼‰
        files = []
        for i in range(50):
            changes = 100  # å„ãƒ•ã‚¡ã‚¤ãƒ«100è¡Œå¤‰æ›´
            files.append(
                FileChange(
                    filename=f"src/module{i}.py",
                    status="modified",
                    additions=50,
                    deletions=50,
                    changes=changes,
                    patch="@@ -1,50 +1,50 @@\n# Refactored code" * 5
                )
            )

        # And: 1000è¡Œã‚’è¶…ãˆã‚‹å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
        large_file = FileChange(
            filename="src/large_module.py",
            status="modified",
            additions=800,
            deletions=800,
            changes=1600,
            patch="@@ -1,800 +1,800 @@\n# Large refactoring"
        )

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æº–å‚™
        token_estimator = TokenEstimator(logger=logger)
        statistics = PRCommentStatistics(token_estimator=token_estimator, logger=logger)
        formatter = CommentFormatter(logger=logger)

        # When: ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ã‚’å®Ÿè¡Œ
        # Step 1: å¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
        processed_files = []
        skipped_files = []
        for file in files + [large_file]:
            if file.changes > 1000:
                skipped_files.append(file)
            else:
                processed_files.append(file)

        # Step 2: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        chunk_size = statistics.calculate_optimal_chunk_size(processed_files, max_tokens=3000)

        # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [processed_files[i:i + chunk_size] for i in range(0, len(processed_files), chunk_size)]

        # Then: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã‚‹
        assert len(chunks) > 1

        # Step 4: å„ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†æï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        chunk_analyses = [
            f"ãƒãƒ£ãƒ³ã‚¯ {i+1}: {len(chunk)} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã—ãŸã€‚"
            for i, chunk in enumerate(chunks)
        ]

        # And: å„ãƒãƒ£ãƒ³ã‚¯ãŒå€‹åˆ¥ã«åˆ†æã•ã‚Œã‚‹
        assert len(chunk_analyses) == len(chunks)

        # Step 5: ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        stats = statistics.calculate_statistics(processed_files)
        summary = f"å¤§è¦æ¨¡ãªãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°PRã§ã™ã€‚{stats['file_count']} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†ã•ã‚Œã¾ã—ãŸã€‚"

        # Step 6: æœ€çµ‚ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        comment = formatter.format_final_comment(summary, chunk_analyses, processed_files, skipped_files)

        # And: ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ãŒã‚³ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹
        assert "âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«" in comment
        assert "src/large_module.py" in comment
        assert len(skipped_files) > 0

    def test_scenario_äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ãŸPRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ(self, logger, temp_template_dir):
        """
        Scenario: æ—§ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã§ã®PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ

        Given: æ—¢å­˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ—§ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹
        When: æ—¢å­˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
        Then: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹
          And: éæ¨å¥¨è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹
          And: PRã‚³ãƒ¡ãƒ³ãƒˆãŒæ­£ã—ãç”Ÿæˆã•ã‚Œã‚‹
        """
        import warnings
        import importlib
        import pr_comment_generator

        # è­¦å‘Šã‚’ã‚­ãƒ£ãƒƒãƒ
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")

            # Given: æ—§ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼ˆå†èª­ã¿è¾¼ã¿ã§éæ¨å¥¨è­¦å‘Šã‚’ç¢ºå®Ÿã«æ•æ‰ï¼‰
            importlib.reload(pr_comment_generator)
            from pr_comment_generator import PRInfo, FileChange, CommentFormatter

            # PRæƒ…å ±ã‚’ä½œæˆ
            pr_info = PRInfo.from_json({
                "title": "Legacy PR",
                "number": 999,
                "body": "Using legacy import path",
                "user": {"login": "legacyuser"},
                "base": {"ref": "main", "sha": "aaa"},
                "head": {"ref": "legacy", "sha": "bbb"}
            })

            files = [
                FileChange.from_json({
                    "filename": "legacy.py",
                    "status": "modified",
                    "additions": 10,
                    "deletions": 5,
                    "changes": 15,
                    "patch": "diff"
                })
            ]

            # When: ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ã‚’å®Ÿè¡Œ
            formatter = CommentFormatter(logger=logger)
            comment = formatter.format_final_comment(
                summary="Legacy PR summary",
                chunk_analyses=["Legacy analysis"],
                files=files
            )

            # Then: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹
            assert comment is not None
            assert "# å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼" in comment
            assert "legacy.py" in comment

            # And: éæ¨å¥¨è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹
            deprecation_warnings = [warning for warning in w if issubclass(warning.category, DeprecationWarning)]
            assert len(deprecation_warnings) >= 1

    def test_scenario_ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰_çµ±è¨ˆã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¾ã§(self, logger, temp_template_dir):
        """
        Scenario: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ•ãƒ­ãƒ¼

        Given: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPRæƒ…å ±ã¨ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã—ã¦ã„ã‚‹
        When: çµ±è¨ˆè¨ˆç®—â†’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²â†’åˆ†æâ†’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹
        Then: æœ€çµ‚çš„ãªå®Œå…¨ãªPRã‚³ãƒ¡ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹
          And: ã‚³ãƒ¡ãƒ³ãƒˆã«ã™ã¹ã¦ã®å¿…è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹
        """
        # Given: ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        pr_info = PRInfo.from_json({
            "title": "Comprehensive Feature Implementation",
            "number": 777,
            "body": "This PR implements a comprehensive feature with tests and documentation",
            "user": {"login": "fullstackdev"},
            "base": {"ref": "main", "sha": "base123"},
            "head": {"ref": "feature/comprehensive", "sha": "head456"}
        })

        files = [
            FileChange(filename="src/feature.py", status="added", additions=200, deletions=0, changes=200, patch="new feature"),
            FileChange(filename="src/helper.py", status="modified", additions=50, deletions=20, changes=70, patch="helper updates"),
            FileChange(filename="tests/test_feature.py", status="added", additions=150, deletions=0, changes=150, patch="new tests"),
            FileChange(filename="docs/feature.md", status="added", additions=100, deletions=0, changes=100, patch="documentation"),
            FileChange(filename="README.md", status="modified", additions=20, deletions=5, changes=25, patch="readme updates"),
        ]

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æº–å‚™
        token_estimator = TokenEstimator(logger=logger)
        statistics = PRCommentStatistics(token_estimator=token_estimator, logger=logger)
        formatter = CommentFormatter(logger=logger)

        # When: å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        # Step 1: çµ±è¨ˆè¨ˆç®—
        stats = statistics.calculate_statistics(files)

        # Step 2: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—ã¨åˆ†å‰²
        chunk_size = statistics.calculate_optimal_chunk_size(files, max_tokens=3000)
        chunks = [files[i:i + chunk_size] for i in range(0, len(files), chunk_size)]

        # Step 3: ãƒãƒ£ãƒ³ã‚¯åˆ†æï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            analysis = f"ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®åˆ†æ:\n"
            for file in chunk:
                analysis += f"  - {file.filename}: {file.status} (+{file.additions} -{file.deletions})\n"
            chunk_analyses.append(analysis)

        # Step 4: ã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = (
            f"ã“ã®PRã¯ {pr_info.title} ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚\n\n"
            f"**çµ±è¨ˆæƒ…å ±:**\n"
            f"- ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['file_count']}\n"
            f"- è¿½åŠ è¡Œæ•°: {stats['total_additions']}\n"
            f"- å‰Šé™¤è¡Œæ•°: {stats['total_deletions']}\n"
            f"- åˆè¨ˆå¤‰æ›´è¡Œæ•°: {stats['total_changes']}\n"
        )

        # Step 5: æœ€çµ‚ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        comment = formatter.format_final_comment(summary, chunk_analyses, files)

        # Then: å®Œå…¨ãªã‚³ãƒ¡ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹
        assert comment is not None
        assert len(comment) > 0

        # And: ã™ã¹ã¦ã®å¿…è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        assert "# å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼" in comment

        # ã‚µãƒãƒªãƒ¼æƒ…å ±
        assert pr_info.title in comment
        assert "çµ±è¨ˆæƒ…å ±" in comment
        assert str(stats['file_count']) in comment
        assert str(stats['total_additions']) in comment

        # ãƒãƒ£ãƒ³ã‚¯åˆ†æ
        for i in range(len(chunks)):
            assert f"ãƒãƒ£ãƒ³ã‚¯ {i+1}" in comment

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        assert "## å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«" in comment
        for file in files:
            assert file.filename in comment

        # çµµæ–‡å­—
        assert "âœ¨" in comment  # added files
        assert "ğŸ“" in comment  # modified files
