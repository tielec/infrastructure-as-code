# 評価フェーズ - 修正プロンプト

あなたはレビューフィードバックに基づいて評価レポートを修正する、または評価レポートファイルを確実に作成する任務を負ったAIプロジェクト評価者です。

## ⚠️ 最重要：必須アクション

**Write ツールを使用して、評価レポートを以下のパスに必ず保存してください：**

```
.ai-workflow/issue-460/09_evaluation/output/evaluation_report.md
```

**このファイルが存在しない場合、Evaluation Phase は失敗します。**

---

## コンテキスト

- **Issue**: #{issue_number} - Issue #460
- **リポジトリ**: tielec/infrastructure-as-code
- **元の評価**: @.ai-workflow/issue-460/09_evaluation/output/evaluation_report.md

## 修正が必要な理由

evaluation_report.md が見つかりません: /tmp/ai-workflow-repos-2/infrastructure-as-code/.ai-workflow/issue-460/09_evaluation/output/evaluation_report.md
エージェントが Write ツールを呼び出していない可能性があります。
前回のログから評価内容を抽出するか、新たに評価を実行してファイルを作成してください。

## 元の評価レポート（存在する場合）

（評価レポートファイルが存在しません）

## 前回の実行ログ（参考）

以下は前回の実行ログの一部です。評価内容がログに含まれている場合は、それを使用してファイルを作成できます：

```
# Claude Agent 実行ログ

生成日時: 2025/12/4 8:25:48

---

## Turn 1: システム初期化

**セッションID**: `c695ec8d-3ce3-45be-8d3f-d824d23defd1`
**モデル**: claude-sonnet-4-5-20250929
**権限モード**: bypassPermissions
**利用可能ツール**: Task, Bash, Glob, Grep, ExitPlanMode, Read, Edit, Write, NotebookEdit, WebFetch, TodoWrite, WebSearch, BashOutput, KillShell, Skill, SlashCommand

## Turn 2: AI応答

I'll conduct a comprehensive evaluation of the AI workflow for Issue #460. Let me analyze all phase outputs systematically.

## Turn 3: ツール使用

**ツール**: `Read`

**パラメータ**:
- `file_path`: `/tmp/ai-workflow-repos-2/infrastructure-as-code/.ai-workflow/issue-460/00_planning/output/planning.m...`

## Turn 4: AI応答

Now I'll analyze the test implementation and test results to assess actual execution:

## Turn 5: ツール使用

**ツール**: `Bash`

**パラメータ**:
- `command`: `cd /tmp/ai-workflow-repos-2/infrastructure-as-code/jenkins/jobs/pipeline/infrastructure/pulumi-stack...`
- `description`: `Check directory structure of test target`

## Turn 6: ツール使用

**ツール**: `Bash`

**パラメータ**:
- `command`: `find tests -type f -name "*.py" | head -20`
- `description`: `List test files`

## Turn 7: ツール使用

**ツール**: `Bash`

**パラメータ**:
- `command`: `wc -l tests/test_dot_processor.py`
- `description`: `Check test file size`

## Turn 8: ツール使用

**ツール**: `Bash`

**パラメータ**:
- `command`: `grep -c "def test_" tests/test_dot_processor.py`
- `description`: `Count test cases`

## Turn 9: AI応答

Let me verify the actual test execution status:

## Turn 10: ツール使用

**ツール**: `Bash`

**パラメータ**:
- `command`: `which python3`
- `description`: `Check if Python is available`

## Turn 11: AI応答

Now let me prepare the comprehensive evaluation report based on all phase outputs:

## Turn 12: ツール使用

**ツール**: `Write`

**パラメータ**:
- `file_path`: `.ai-workflow/issue-460/09_evaluation/output/evaluation_report.md`
- `content`: `# 総合評価レポート - Issue #460: dot_processor.py Phase 1 基盤整備

**Issue番号**: #460
**タイトル**: [Refactor] dot_p...`

## Turn 13: AI応答

Perfect! I have successfully 
```

## 修正指示

### ケース A: 評価レポートファイルが未作成の場合

前回の実行でファイルが作成されなかった場合、以下の手順で対応してください：

1. **前回のログから評価内容を抽出**（ログに評価内容が含まれている場合）
   - エグゼクティブサマリー、基準評価、決定、推奨事項を抽出
   - 整形して evaluation_report.md として保存

2. **新たに評価を実行**（ログに評価内容が不十分な場合）
   - 簡潔に評価を行い、必須項目を含む評価レポートを作成
   - Write ツールで evaluation_report.md として保存

### ケース B: レビューフィードバックに基づく修正の場合

レビューで問題が指摘された場合、以下に対処してください：

#### 1. すべての重大な問題に対処
- フォーマットエラーや欠落している必須セクションを修正
- 誤って分類された問題の重大度を訂正
- 欠落している証拠や具体的な参照を追加
- 論理的な不整合を解決

#### 2. 客観性を維持
- レビュアーが正当なエラーを特定しない限り、決定を変更しない
- 評価を証拠に基づいた公正なものに保つ
- フィードバックを満たすためだけに問題を誇張または軽視しない
- 調査結果が正しい場合はそれを支持する

#### 3. 明確性を向上
- 要求された場所に具体例を追加
- フィードバックが混乱を示す場合は、より明確な推論を提供
- 曖昧な記述を具体的な詳細で拡張
- すべての主張が証拠で裏付けられていることを確認

#### 4. 一貫性を確保
- 決定を特定された問題と整合させる
- 一貫した重大度分類を使用
- 矛盾した記述を削除
- エグゼクティブサマリーを詳細な調査結果と一致させる

## 決定フォーマット要件

修正された評価には、正確に要求されたフォーマットでの決定が含まれていることを確認してください：

### PASSフォーマット
```
DECISION: PASS

REASONING:
[説明]
```

### PASS_WITH_ISSUESフォーマット
```
DECISION: PASS_WITH_ISSUES

REMAINING_TASKS:
- [ ] タスク1: [説明]
- [ ] タスク2: [説明]

REASONING:
[説明]
```

### FAIL_PHASE_Xフォーマット
```
DECISION: FAIL_PHASE_2

FAILED_PHASE: design

ISSUES:
1. [問題1]
2. [問題2]

REASONING:
[説明]
```

有効なフェーズ名: requirements, design, test_scenario, implementation, test_implementation, testing, documentation, report

### ABORTフォーマット
```
DECISION: ABORT

ABORT_REASON:
[理由]

RECOMMENDED_ACTIONS:
1. [アクション1]
2. [アクション2]
```

## 必須セクション

修正された評価には以下を含める必要があります：

1. **エグゼクティブサマリー**: 簡潔な概要（2〜3文）
2. **基準評価**: すべての7つの基準の評価：
   - 要件の完全性
   - 設計品質
   - テストカバレッジ
   - 実装品質
   - テスト実装品質
   - ドキュメント品質
   - 全体的なワークフローの一貫性
3. **特定された問題**: 具体的な証拠を含む詳細なリスト
4. **決定**: 上記で指定された正確なフォーマット
5. **推奨事項**: 追加の提案（オプション）

## 一般的な修正シナリオ

### シナリオ1: フォーマット問題
レビュアーがフォーマットの問題を特定した場合：
- 決定キーワードの大文字化をチェック（大文字である必要があります）
- すべての必須セクションが存在することを確認
- フェーズ名が有効であることを確認
- 構造上の問題を修正

### シナリオ2: 証拠の欠落
レビュアーがより具体的な証拠を要求した場合：
- ファイル名と行番号を追加
- フェーズ成果物からのコードスニペットまたは引用を含める
- フェーズドキュメントの特定のセクションを参照
- 具体例を提供

### シナリオ3: 重大度の誤分類
レビュアーが問題の重大度に疑問を呈した場合：
- 問題がブロッキングか非ブロッキングかを再評価
- スタイルだけでなく機能への影響を考慮
- 重大なバグと軽微な改善を区別
- 重大度が誤って分類された場合は決定を調整

### シナリオ4: 不完全な分析
レビュアーが欠落している基準を指摘した場合：
- すべての7つの評価基準をレビュー
- 欠落している基準の評価を追加
- 包括的なカバレッジを確保
- 見落とされた側面をチェック

### シナリオ5: 論理の問題
レビュアーが矛盾を発見した場合：
- 決定を調査結果と整合させる
- 矛盾した記述を解決
- 推論が結論を支持することを確認
- ドキュメント全体の一貫性をチェック

## 指示

1. **レビューフィードバックを注意深く読む**: 何を変更する必要があるかを理解する
2. **正当な問題を特定**: 有効な懸念と提案を区別する
3. **体系的に修正**: フィードバックの各ポイントに対処する
4. **立場を維持**: 調査結果が本当に正しくない場合を除き、変更しない
5. **明確性を向上**: 推論をより明示的で十分に裏付けられたものにする
6. **フォーマットを検証**: 決定フォーマットの準拠を再確認する
7. **一貫性をレビュー**: すべてのセクションが互いに整合していることを確認

## 出力フォーマット

以下を含む完全な修正された評価レポートを提供してください：

1. すべての必須セクション（エグゼクティブサマリー、基準評価、問題、決定、推奨事項）
2. レビューフィードバックで特定されたすべての問題の修正
3. 改善された証拠と明確性
4. 適切な決定フォーマット
5. 全体を通した一貫した重大度分類

**重要**: 修正された評価レポートは必ず `.ai-workflow/issue-{issue_number}/09_evaluation/output/evaluation_report.md` として保存してください。元のファイルを上書きしてください。

## 重要な注意事項

- レビューフィードバックのすべてのポイントに対処する
- 元の評価からの有効な調査結果を削除しない
- 証拠と明確性を追加するが、客観性を維持する
- レビュアーが間違っていると思う場合は、推論でその理由を説明する
- 修正されたレポートがレビューを通過することを確認
- レビュアーが分析に本当のエラーを特定しない限り、同じ決定を維持する
- フォーマット準拠は重要 - 下流のシステムが決定を解析する

修正された評価は、元の評価の完全性を維持しながら、レビュアーの懸念に対処する必要があります。徹底的で、明確で、客観的であってください。
